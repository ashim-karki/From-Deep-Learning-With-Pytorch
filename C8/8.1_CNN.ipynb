{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a6d2474b70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../C7'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Conv1d = for time series\n",
    "# nn.Conv2d = for images\n",
    "# nn.Conv3d = for volumes or videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " torch.Size([16, 3, 3, 3]),\n",
       " torch.Size([16]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of conv\n",
    "\n",
    "conv = nn.Conv2d(3, 16, kernel_size=3, padding=1) # channels, number of output features, size of kernel aka filter\n",
    "conv, conv.weight.shape, conv.bias.shape\n",
    "# weight shape = outputs x channels x filtersize (padding doesn't change weight shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vertical edge detection filter/kernel example\n",
    "\n",
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                    [-1.0, 0.0, 1.0],\n",
    "                                    [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()\n",
    "\n",
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maxpool example\n",
    "\n",
    "pool = nn.MaxPool2d(2) # choses max from 2x2 tiles\n",
    "output = pool(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the second 3 × 3 convolution kernel produces 21 in its conv output in figure\n",
    "8.8, this is based on the top-left 3 × 3 pixels of the first max pool output. They, in turn,\n",
    "correspond to the 6 × 6 pixels in the top-left corner in the first conv output, which in\n",
    "turn are computed by the first convolution from the top-left 7 × 7 pixels. So the pixel\n",
    "in the second convolution output is influenced by a 7 × 7 input square. The first\n",
    "convolution also uses an implicitly “padded” column and row to produce the output in\n",
    "the corner; otherwise, we would have an 8 × 8 square of input pixels informing a given\n",
    "pixel (away from the boundary) in the second convolution’s output. In fancy language,\n",
    "we say that a given output neuron of the 3 × 3-conv, 2 × 2-max-pool, 3 × 3-conv\n",
    "construction has a receptive field of 8 × 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When moving from conv layers -> FC layers...\n",
    "##### What’s missing there is the reshaping step from an 8-channel 8 × 8 image to a 512-element, 1D vector (1D if we ignore the batch dimension, that is). This could be achieved by calling view on the output of the last nn.MaxPool2d, but unfortunately, we don’t have any explicit visibility of the output of each module when we use nn.Sequential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution...\n",
    "\n",
    "##### At some point in developing neural networks, we will find ourselves in a situation where we want to compute something that the premade modules do not cover. Here, it is something very simple like reshaping. When we want to build models that do more complex things than just applying one layer after another, we need to leave nn.Sequential for something that gives us added flexibility. PyTorch allows us to use any computation in our model by subclassing nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again: When going from conv layer to FC layer that output is not defined in sequential layer for each layer \n",
    "# so this causes problem for change in 3d -> 1d. Hence another module is created that accounts the output for \n",
    "# each layer\n",
    "\n",
    "# This solution can be used for many problems. Here this could also be solved by:\n",
    "\n",
    "flatten = nn.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constructor __init__\n",
    "# assign self to forward function\n",
    "# Forward function will hold the parameters throughout the lifetime of module\n",
    "# To hold these parameters remember to super().__init__()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8*8*8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make this more concise we use functional\n",
    "\n",
    "# By “functional” here we mean “having no internal state”—in other words, “whose output value is solely \n",
    "# and fully determined by the value input arguments unlike nn which works on input arguments and stored parameters.” \n",
    "\n",
    "# We can safely switch to the functional counterparts of pooling and activation, since they have no parameters\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # to hold the parameters \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8*8*8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8*8*8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "# When we are writing our own forwards, it may be more natural to\n",
    "# use the functional interface for things that do not need state in the form of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0604, -0.0565]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs+1): # loop starts from 1 to n_epochs+1\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader: # loops over batches\n",
    "            outputs = model(imgs) # feeds a batch through model\n",
    "            loss = loss_fn(outputs, labels) # computes loss\n",
    "            optimizer.zero_grad() # getting rid of gradients from last round\n",
    "            loss.backward() # evaluated gradient of parameters\n",
    "            optimizer.step() # update the model\n",
    "            loss_train += loss.item() # sums the loses over the epoch. .item() escapes the gradient\n",
    "        if epoch == 1 or epoch % 2 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "            datetime.datetime.now(), epoch,\n",
    "            loss_train / len(train_loader))) # divide the lenght to get the average loss per batch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-27 15:42:44.607877 Epoch 1, Training loss 0.5775922356517451\n",
      "2024-04-27 15:42:45.790877 Epoch 2, Training loss 0.49039286716728453\n",
      "2024-04-27 15:42:48.231877 Epoch 4, Training loss 0.44058112297088475\n",
      "2024-04-27 15:42:50.648877 Epoch 6, Training loss 0.390806076063472\n",
      "2024-04-27 15:42:53.080879 Epoch 8, Training loss 0.35277803878116\n",
      "2024-04-27 15:42:55.537877 Epoch 10, Training loss 0.33541781317656205\n",
      "2024-04-27 15:42:57.993877 Epoch 12, Training loss 0.32785301679258894\n",
      "2024-04-27 15:43:00.488877 Epoch 14, Training loss 0.3188092040408189\n",
      "2024-04-27 15:43:03.038877 Epoch 16, Training loss 0.3127938439701773\n",
      "2024-04-27 15:43:05.507876 Epoch 18, Training loss 0.3067388805993803\n",
      "2024-04-27 15:43:08.156877 Epoch 20, Training loss 0.3005283106664184\n",
      "2024-04-27 15:43:10.906877 Epoch 22, Training loss 0.2950754595600116\n",
      "2024-04-27 15:43:13.761877 Epoch 24, Training loss 0.28879969676209105\n",
      "2024-04-27 15:43:16.298878 Epoch 26, Training loss 0.28384472192472715\n",
      "2024-04-27 15:43:19.002877 Epoch 28, Training loss 0.27736751744701604\n",
      "2024-04-27 15:43:21.598877 Epoch 30, Training loss 0.27277232345881736\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=30,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.89\n",
      "Accuracy val: 0.87\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # no use of gradients here\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # index for highest value of output\n",
    "                total += labels.shape[0]  # increased by batch size\n",
    "                correct += int((predicted == labels).sum())  # number of true predictions\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the model\n",
    "\n",
    "# torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')\n",
    "\n",
    "# to load the model \n",
    "\n",
    "# loaded_model = Net()\n",
    "# loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "# Move tensors and parameters to the GPU\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensors\n",
    "\n",
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)  # Here the image tensors are moved to the GPU\n",
    "            labels = labels.to(device=device) # Here the labels tensors are moved to the GPU\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-27 15:44:18.055684 Epoch 1, Training loss 0.564361229254182\n",
      "2024-04-27 15:44:20.827684 Epoch 10, Training loss 0.3302257911414857\n",
      "2024-04-27 15:44:23.885683 Epoch 20, Training loss 0.2985191607171563\n",
      "2024-04-27 15:44:26.943683 Epoch 30, Training loss 0.2720786065907235\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = Net().to(device=device)  # Here the parametes/model is moved to the GPU. .cuda() can also be used if sure there is a graphics card\n",
    "\n",
    "# Moves our model (all parameters) to the GPU. If you forget to move either the \n",
    "# model or the inputs to the GPU, you will get errors about tensors not being on the same \n",
    "# device, because the PyTorch operators do not support mixing GPU and CPU inputs.\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 30,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = Net().to(device=device)\n",
    "# loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization: This term is crafted so that the weights of the model tend to be small on their own, limiting\n",
    "# how much training makes them grow. In other words, it is a penalty on larger weight values. \n",
    "\n",
    "# L2 regularization: Sum of squares of all weight\n",
    "# L1 regularization: Sum of absolute values of weights\n",
    "\n",
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
    "                        train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                          for p in model.parameters())  # replace pow(2.0) with abs() for L1 regularization\n",
    "            loss = loss + l2_lambda * l2_norm # added to loss that is backpropagated\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader))) \n",
    "            \n",
    "# SGD already has weight_decay parameter that correspoinds to 2*lambda which can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea behind dropout: zero out a random fraction of outputs from neurons across the network,\n",
    "# where the randomization happens at each training iteration.\n",
    "\n",
    "# Less chance of coordination in the memorization process\n",
    "\n",
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "# During evaluation the dropout is bypassed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, \n",
    "                               padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "# if model.eval() is used then normalization layers are forzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In regard to computer vision, a shallower network could identify a person’s shape in a photo, \n",
    "# whereas a deeper network could identify the person, the face on their top half, and the mouth within the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a large network of more than 20 layers, the backpropagation multiplication can cause generation of small or large uneven number\n",
    "# Hence resnet was used for skip connections\n",
    "\n",
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2) # identity mapping done, creates a direct path from deeper parameters to the loss. This makes the contribution to the gradient more direct\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEEP MODELS: Creating 1 block\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)  # <1>\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                      nonlinearity='relu')  # <2>\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEEP MODELS: Creating Deep model using many blocks\n",
    "\n",
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
